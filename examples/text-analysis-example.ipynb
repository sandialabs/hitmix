{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python HITMIX Example: Text Analysis\n",
    "\n",
    "In this notebook, we demonstrate the use of HITMIX for computing _hitting time moments_ for a set of vertices in a text analysis application.\n",
    "\n",
    "```\n",
    "Copyright 2021 National Technology & Engineering Solutions of Sandia,\n",
    "LLC (NTESS). Under the terms of Contract DE-NA0003525 with NTESS, the\n",
    "U.S. Government retains certain rights in this software.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "* Danny Dunlavy (dmdunla@sandia.gov)\n",
    "* Peter Chew (pachew@galiteoconsulting.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load package dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# NLTK -- used in Gensim for the Lemmatizer and Stemmers. \n",
    "import nltk\n",
    "\n",
    "# Regular expressions\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data and work directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For local import of the data\n",
    "data_dir_name    = \"./data/\"\n",
    "figures_dir_name = \"./figures/\"\n",
    "models_dir_name  = \"./models/\"\n",
    "\n",
    "dirs = [data_dir_name, figures_dir_name, models_dir_name]\n",
    "for dr in dirs: \n",
    "    if not os.path.exists(dr):\n",
    "        os.makedirs(dr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data into Pandas DataFrame\n",
    "See [README.md](/edit/README.md) file for information about downloading and using these datasets.\n",
    "\n",
    "NOTE: Uncomment lines of datasets you want to use; comment lines of datasets you do not want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiline ASCII text\n",
    "corpus_filename = \"multiline-ascii.txt\" \n",
    "df = pd.read_csv(data_dir_name + corpus_filename, header=None, names=['text'])\n",
    "# replace line feeds with spaces in multiline text\n",
    "df['text'] = df['text'].str.replace('\\n',' ')\n",
    "df['text'] = df['text'].str.replace('\\r',' ')\n",
    "\n",
    "# # UTF-8 data in multiple languages\n",
    "# corpus_filename = \"language-learning-and-teaching.txt\" \n",
    "# df = pd.read_csv(data_dir_name + corpus_filename, header=None, sep='\\n', names=['text'])\n",
    "\n",
    "# # Twitter BBC health news headlines\n",
    "# corpus_filename = \"bbchealth.txt\" \n",
    "# df = pd.read_csv(data_dir_name + corpus_filename, header=None, sep='|', names=['id','date','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Document-Term Matrix using the Sklearn CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# preprocessor to remove various unwanted terms\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    # remove URLs\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',text)\n",
    "    # remove numbers\n",
    "    #text = re.sub(r'\\d+', '', text)\n",
    "    return text\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# create document-term matrix\n",
    "corpus = df[\"text\"]\n",
    "# Alternative: strip accents, min document frequency = 1%, max document frequency = 75%\n",
    "#vectorizer = CountVectorizer(preprocessor=preprocess_text, strip_accents='ascii', min_df=int(.01*len(df)), max_df=int(.75*len(df)))\n",
    "vectorizer = CountVectorizer(preprocessor=preprocess_text)\n",
    "A = vectorizer.fit_transform(corpus)\n",
    "\n",
    "end = time.time()\n",
    "fit_transform_time = end-start\n",
    "print(f\"It took {end-start} seconds to compute the document-term matrix\")\n",
    "print(f\"A: {A.shape}\")\n",
    "dictionary = np.array(vectorizer.get_feature_names())\n",
    "# print vocabulary as a dictionary (term : index)\n",
    "#print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pointwise Mutual Information (PMI) between terms and documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_pmi(A):\n",
    "    # pmi(i,j) = log2( p(i,j)/p(i)/p(j) ) for term i in document j\n",
    "\n",
    "    # probability of term i over corpus: p(i)\n",
    "    term_sums = A.sum(axis=0)\n",
    "    total_terms = term_sums.sum()\n",
    "    p_i = term_sums / total_terms\n",
    "\n",
    "    # probability of term i over corpus: p(i)\n",
    "    doc_sums = A.sum(axis=1)\n",
    "    p_j = doc_sums / doc_sums.sum() \n",
    "\n",
    "    # weighted matrix\n",
    "    A_pmi = A.copy()\n",
    "    A_pmi.data = A_pmi.data.astype(float)\n",
    "    pmi= (A_pmi/total_terms)/p_i/p_j\n",
    "    A_pmi[A.nonzero()] = np.log2(pmi[A.nonzero()])\n",
    "    \n",
    "    return A_pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate matrix_pmi\n",
    "\n",
    "# Validation uses data from Table 2 in the following journal article:\n",
    "# CHEW, P., BADER, B., HELMREICH, S., ABDELALI, A., & VERZI, S. (2011). \n",
    "# An information-theoretic, vector-space-model approach to cross-language \n",
    "# information retrieval. Natural Language Engineering, 17(1), 37-70. \n",
    "# doi:10.1017/S1351324910000185\n",
    "Atest_term_counts = csr_matrix([[2,1,1,1,0,0,0,0],[3,1,0,1,1,1,1,0],[2,0,0,1,0,1,1,1]])\n",
    "\n",
    "# Compute PMI\n",
    "Atest_pmi = matrix_pmi(Atest_term_counts)\n",
    "\n",
    "# Round the data to 3 digits of accuracy\n",
    "Atest_pmi.data = np.round(Atest_pmi.data,3)\n",
    "\n",
    "# Convert to dense term-document matrix (i.e., transpose)\n",
    "Atest_pmi_final = Atest_pmi.T.todense()\n",
    "\n",
    "# Table 3 data from the same journal article:\n",
    "Atrue_pmi = np.array([[ 0.119,  0.026, -0.144],\n",
    "                      [ 0.926,  0.248,  0.   ],\n",
    "                      [ 1.926,  0.,     0.   ],\n",
    "                      [ 0.341, -0.337,  0.078],\n",
    "                      [ 0.,     1.248,  0.   ],\n",
    "                      [ 0.,     0.248,  0.663],\n",
    "                      [ 0.,     0.248,  0.663],\n",
    "                      [ 0.,     0.,     1.663]])\n",
    "\n",
    "# check if they are the same\n",
    "if np.linalg.norm(Atest_pmi_final-Atrue_pmi) == 0:\n",
    "    print(\"The computed and published PMI values match!\")\n",
    "else:\n",
    "    print(\"The computed and published PMI values do not match!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute PMI and show the min and max values\n",
    "A_pmi = matrix_pmi(A)\n",
    "print(f\"Min PMI: {np.min(A_pmi)}, Max PMI: {np.max(A_pmi)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot histogram of PMI weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(A_pmi.data, log=True, bins=100)\n",
    "plt.xlabel('PMI')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute a truncated singular value decomposition (SVD) of the weighted document-term matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# user-defined topics (i.e., rank of SVD)\n",
    "num_topics = min([min([A_pmi.shape[0], A_pmi.shape[1]])-1, 100])\n",
    "print(f\"Number of topics to compute: {num_topics}\")\n",
    "\n",
    "# compute SVD\n",
    "U,S,Vt = svds(A_pmi, k=num_topics)\n",
    "\n",
    "# reorder SVD based on decreasing singular values\n",
    "U[:,:num_topics] = U[:, num_topics-1::-1]\n",
    "S = S[::-1]\n",
    "Vt[:num_topics, :] = Vt[num_topics-1::-1, :]\n",
    "\n",
    "# plot distribution of singular values\n",
    "plt.figure()\n",
    "plt.plot(range(len(S)), S, '.')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Singular Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot explained variance for each topic (over all data and topics)\n",
    "\n",
    "_Note:_ For large document-term matrices, the following computations may take a very long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ufull,Sfull,Vtfull = svds(A_pmi, k=min(A_pmi.shape)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_sv = Sfull.sum()\n",
    "# plt.plot(range(len(Sfull)), 100*Sfull[::-1]/sum_sv, 'ko')\n",
    "# plt.xlabel('Index of topic')\n",
    "# plt.ylabel('Percentage of explained variance')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_sv = Sfull.sum()\n",
    "# plt.plot(range(len(Sfull)), 100*Sfull[::-1].cumsum()/sum_sv, 'ko')\n",
    "# plt.xlabel('Index of topic')\n",
    "# plt.ylabel('Cummulative percentage of explained variance')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic-Term-Document Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_terms(Vt, t, n):\n",
    "    # get term vector for topic t\n",
    "    term_dist = Vt[t, :]\n",
    "    # sort the term vector weights\n",
    "    sort_indices = np.flip(term_dist.argsort())\n",
    "    terms = dictionary[sort_indices][:n]\n",
    "    weights = term_dist[sort_indices][:n]\n",
    "    # return top n terms as a list\n",
    "    return terms, weights\n",
    "\n",
    "def get_topic_documents(U, t, n):\n",
    "    # get document vector for topic t\n",
    "    doc_dist = U[:, t]\n",
    "    # sort the document vector weights\n",
    "    sort_indices = np.flip(doc_dist.argsort())\n",
    "    docs = corpus[sort_indices][:n]\n",
    "    weights = doc_dist[sort_indices][:n]\n",
    "    # return top n docs as a list\n",
    "    return docs, weights\n",
    "\n",
    "def get_document_topics(U, d):\n",
    "    # get topic vector for document d\n",
    "    doc_dist = U[d, :]\n",
    "    # return topic vector\n",
    "    return doc_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot topic-term distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = A.shape[1]\n",
    "topics_to_plot = [0,1]\n",
    "plt.figure()\n",
    "for i in topics_to_plot:\n",
    "    terms, weights = get_topic_terms(Vt, i, N)\n",
    "    plt.plot(weights,'.')\n",
    "plt.legend([\"Topic \"+str(i) for i in topics_to_plot],bbox_to_anchor=(1,1),loc='upper left')\n",
    "plt.ylabel(\"term weight\")\n",
    "plt.xlabel(\"term index\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot document-topic distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = A.shape[0]\n",
    "docs_to_plot = [0,1,2]\n",
    "plt.figure()\n",
    "for i in docs_to_plot:\n",
    "    doc_dist = get_document_topics(U, i)\n",
    "    plt.plot(doc_dist,'.')\n",
    "plt.legend([\"Document \"+str(i) for i in docs_to_plot],bbox_to_anchor=(1,1),loc='upper left')\n",
    "plt.ylabel(\"topic weight\")\n",
    "plt.xlabel(\"topic index\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot topic-document distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "num_docs = U.shape[0]\n",
    "doc_list = np.array(range(num_docs))\n",
    "colors = [\"black\", \"red\"]\n",
    "# all docs are black\n",
    "color_indices = np.zeros((num_docs))\n",
    "# the last 2 docs are red\n",
    "color_indices[-2:] = 1\n",
    "colormap = matplotlib.colors.ListedColormap(colors)\n",
    "\n",
    "subplot_columns = 4\n",
    "plt.figure(figsize=(20,2*num_topics//subplot_columns + 1))\n",
    "for i in range(num_topics):\n",
    "    plt.subplot(num_topics // subplot_columns + 1, subplot_columns, i + 1)\n",
    "    plt.scatter(doc_list,U[:, i],c=color_indices,cmap=colormap,marker='.')\n",
    "    plt.gca().get_xaxis().set_visible(False)\n",
    "    plt.gca().get_yaxis().set_visible(False)\n",
    "    plt.title(f\"topic {i}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute graph adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag to control normalizing (2-norm unit length) rows of scaled left singular vectors\n",
    "scale_US = True\n",
    "\n",
    "# scale document vectors with singular values\n",
    "US = U@np.diag(np.sqrt(S))\n",
    "\n",
    "# normalize rows of US is requested\n",
    "if (scale_US):\n",
    "    US = US / np.linalg.norm(US,axis=1)[:, np.newaxis]\n",
    "\n",
    "print(f\"Row norms of US: mean={np.mean(np.linalg.norm(US,axis=1)):.4f}; std={np.std(np.linalg.norm(US,axis=1)):.4f}\")\n",
    "    \n",
    "# construct similarity graph weighted adjacency matrix, zero out the negatives\n",
    "Adj = US @ US.T\n",
    "Adj[Adj <= 0] = 0\n",
    "Adj_sparse = csr_matrix(Adj)\n",
    "print(f\"Adjacency shape: {Adj_sparse.shape}\")\n",
    "print(f\"Adjacency nonzeros: {Adj_sparse.nnz}\")\n",
    "print(f\"Adjacency density: {Adj_sparse.nnz/np.prod(Adj_sparse.shape)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot distribution of adjacency matrix edge weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(Adj_sparse.data,log=True, bins=200)\n",
    "plt.xlabel('Edge weight')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshhold adjacency matrix and just save topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful thresholds will depend on scaling of singular vectors used in creating the \n",
    "# adjacency graph. The plot above is useful for finding a useful threshold via visual \n",
    "# inspection of the edge weights.\n",
    "threshold = 0.5\n",
    "Adj_thresh = Adj_sparse.copy()\n",
    "Adj_thresh.data = np.where(Adj_thresh.data < threshold, 0, Adj_thresh.data)\n",
    "Adj_thresh.eliminate_zeros()\n",
    "#Adj_thresh.data[:] = 1 \n",
    "print(f\"Adjacency shape: {Adj_thresh.shape}\")\n",
    "print(f\"Adjacency nonzeros: {Adj_thresh.nnz}\")\n",
    "print(f\"Adjacency density: {Adj_thresh.nnz/np.prod(Adj_thresh.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View top terms and documents per topic \n",
    "\n",
    "Print out a few documents that are the most representative of each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_terms_per_topic_to_display = 10\n",
    "num_docs_per_topic_to_display = 3\n",
    "num_chars_per_doc_to_display = 80\n",
    "include_topic_weights = True\n",
    "\n",
    "for t in range(num_topics):\n",
    "    # get terms and topic weights\n",
    "    terms, weights = get_topic_terms(Vt,t,num_terms_per_topic_to_display)\n",
    "\n",
    "    if include_topic_weights:\n",
    "        # include weights\n",
    "        terms_list = [f'{terms[i]} ({weights[i]:1.3f})' for i in range(len(terms))]\n",
    "        terms_str = f\"{t} ({S[t]:0.3f}): \" + ' '.join(terms_list)\n",
    "    else:\n",
    "        # don't include weights\n",
    "        terms_list = [f'{terms[i]} ' for i in range(len(terms))]\n",
    "        terms_str = f\"{t}: \" + ' '.join(terms_list)\n",
    "    \n",
    "    print(f\"{terms_str}\\n\")\n",
    "    docs, doc_weights = get_topic_documents(U, t, num_docs_per_topic_to_display)\n",
    "    for doc_index, doc in docs.items():\n",
    "        if include_topic_weights:\n",
    "            print(f\"\\tDoc {doc_index} ({U[doc_index,t]:1.3f}) {doc[:num_chars_per_doc_to_display]}\")\n",
    "        else:\n",
    "            print(f\"\\tDoc {doc_index} {doc[:num_chars_per_doc_to_display]}\")\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose subset of documents as the hitting set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually selected after reading documents in different topics above\n",
    "documents_of_interest = sorted([1, 0])\n",
    "print(documents_of_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute mean hitting times to `documents_of_interest` (hitting set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hitmix import hitting_time_moments\n",
    "ETm, cg_info = hitting_time_moments(Adj_thresh, documents_of_interest, maxiter=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide rank order of documents related to hitting set\n",
    "\n",
    "Hitting time moments values of 0 indicate vertices in the hitting set.\n",
    "\n",
    "Hitting time moments values of $\\infty$ indicate vertices not related to (i.e. not reachable in the graph from) the hitting set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rank_order = ETm[:,0].argsort()\n",
    "for i in range(ETm.shape[0]):\n",
    "    print(i, rank_order[i], ETm[rank_order][i,0], corpus[rank_order[i]][0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add mean hitting times to the original DataFrame and print it out\n",
    "df['ETm[0]'] = ETm[:,0]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
